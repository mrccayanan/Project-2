{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# End to End Data Science Project with 2015 US Traffic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a91e27153fee411299a149ab57258948a82f1679"
   },
   "source": [
    "This notebook replicates the end to end machine learning project presented in Chapter 2 of [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291?SubscriptionId=AKIAILSHYYTFIVPWUY6Q&tag=duckduckgo-d-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=1491962291), but adjusts to use the [DOT 2015 US Traffic dataset from Kaggle](https://www.kaggle.com/jboysen/us-traffic-2015) instead of the California house price dataset used in the text.\n",
    "\n",
    "The prediction goal is modest, estimate the volume of traffic flow between 8AM & 9AM on 4 particular Minnesota roadways just outside Minneapolis: I-394, I-494, I-94, and Olsen Memorial HIghway (H55). There are 2 distinct challenges. The first is identifying the stations in the traffic stations data that report traffic conditions on these roadways. The second is building a decent prediction model to estimate traffic volume. A third challenge would be to determine the impact to commute time based on traffic flow which I will save for a later date (you can do this by bringing t[ransit time data from HERE API](https://developer.here.com/).\n",
    "\n",
    "Two things stand out to me in this process. First, *a machine learning project is not linear.* As much as I want this notebook to read as a story, the process was back and forth for days. Explore, learn, go back & revise, explore, learn, go back and revise. While the Hands-On text includes all that back and forth, the notebook below only keeps the final methods. Second, *repeatability in the proces is critical for time-saving*. There is a section on writing custom transformers in SciKit Learn which is unreal. If for no other reason, conducting this exercise was worth it just for discovering those few paragraphs. Onward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77c544b4c57d5eb5a61995e657ae1b4b32d1cbed"
   },
   "source": [
    "1. [Get the Data](#Get the Data) <br>\n",
    "a. [Download Data & Look at Structure](#Download Data & Look at Structure)\n",
    "2. [Create a Test Set](#Create a Test Set) <br>\n",
    "a. [Ensure Test Set is Balanced](#Ensure Test Set is Balanced)\n",
    "3. [Discover and Visualize the Data to Gain Insights](#Discover and Visualize the Data to Gain Insights) <br>\n",
    "a. [Visualising Data](#Visualizing Data) <br>\n",
    "b. [Looking for Correlations](#Looking for Correlations) <br>\n",
    "c. [Experimenting with Attribute Combinations](#Experimenting with Attribute Combinations)\n",
    "4. [Prepare Data for Machine Learning Algorithms](#Prepare Data for Machine Learning Algorithms) <br>\n",
    "a. [Data Cleaning](#Data Cleaning) <br>\n",
    "b. [Handling Text and Categorical Attributes](#Handling Text and Categorical Attributes) <br>\n",
    "c. [Writing Custom Transformer](#Writing Custom Transformer)\n",
    "5. [Feature Scaling](#Feature Scaling) <br>\n",
    "a. [Transformation Pipelines](#Transformation Pipelines)\n",
    "6. [Select and Train a Model](#Select and Train a Model) <br>\n",
    "a. [Finding a Benchmark](#Finding a Benchmark) <br>\n",
    "b. [Training and Evaluating on the Training Set](#Training and Evaluating on the Training Set) <br>\n",
    "c. [Better Evaluation Using Cross-Validation](#Better Evaluation Using Cross-Validation) <br>\n",
    "7. [Fine-Tune Hyperparameters with Grid Search](#Fine-Tune Hyperparameters with Grid Search) <br>\n",
    "8. [Evaluate Your System on the Test Set](#Evaluate Your System on the Test Set) <br>\n",
    "9. [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15822c38b82508b69129cf24644d9777db288705"
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd; import numpy as np\n",
    "\n",
    "# set Jupyter's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# display warnings only the first time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35e6e2510e30b2dd90b3377ee53bd689b00e0825"
   },
   "source": [
    "<a></a>\n",
    "## Get the Data\n",
    "<a></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3aa60dc7393566d8d2e910bf0e727817076b1387"
   },
   "source": [
    "### Download Data & Look at Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24118859721101029f02e43543cf9027826aa533"
   },
   "source": [
    "Start by loading in the traffic and traffic station data. I'm renaming the longest column name so the view is more concise. Then take a look at the top to get a sense of the dataset... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# traffic station characteristics\n",
    "traffic_station_df = pd.read_csv('../input/us-traffic-2015/dot_traffic_stations_2015.txt.gz',\n",
    "                                 header=0, sep=',', quotechar='\"')\n",
    "\n",
    "# traffic volume metrics associated to each traffic station\n",
    "traffic_df = pd.read_csv('../input/us-traffic-2015/dot_traffic_2015.txt.gz',\n",
    "                         header=0, sep=',', quotechar='\"')\n",
    "\n",
    "# rename terribly long feature names\n",
    "traffic_station_df.rename(columns = {\"number_of_lanes_in_direction_indicated\": \"lane_count\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8f5764bb124df5135ecd47af3665c17804683fc"
   },
   "outputs": [],
   "source": [
    "# view top of station dataframe\n",
    "print('Traffic Station data:')\n",
    "traffic_station_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5002dbc976f8f74594b9db03f9dfa64137d6182"
   },
   "outputs": [],
   "source": [
    "# view top of traffic volume dataframe\n",
    "print('Traffic data:')\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13f4aa60769d9bcd755793771a66a0f5fca51456"
   },
   "source": [
    "There is far more information here than I am going to use so at this point I'll scale down the datasets to the information I'll want to use and, if necessary, join the two together. In order to predict traffic volume between 8AM and 9AM, some of the preliminary features that jump to mind are direction of travel, day of week, and weather conditions. We'll need to pull weather data from another source. Note: I'm assuming times are in local time (for our lanes this is CST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "daf920513982d1abc30d058c14ce1695ace26a40"
   },
   "outputs": [],
   "source": [
    "# specify the features we'll want going forward\n",
    "station_vars = [\"direction_of_travel\", \"fips_county_code\", \"fips_state_code\",\n",
    "                \"lane_of_travel\", \"lane_count\", \"latitude\", \"longitude\", \n",
    "                \"station_id\", \"station_location\", \"type_of_sensor\", \"year_of_data\",\n",
    "                \"year_station_established\"]\n",
    "\n",
    "traffic_vars = [\"date\", \"day_of_data\", \"day_of_week\", \"direction_of_travel\",\n",
    "                \"fips_state_code\", \"lane_of_travel\", \"month_of_data\", \"record_type\",\n",
    "                \"restrictions\", \"station_id\", \"traffic_volume_counted_after_0800_to_0900\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "280b6b16297a0446eb745d4b8f751fe8fc39f5e6"
   },
   "source": [
    "We certainly want to join on station_id and direction_of_travel. However, since I'm only interested in 4 roadways I want to filter the data prior to joining (I imagine a situation where the data is much larger and join much costlier). So, [FIPS state code for Minnesota is 27](https://www.mcc.co.mercer.pa.us/dps/state_fips_code_listing.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5e3c427601142df232dd4a04857b12e0b241aa1"
   },
   "outputs": [],
   "source": [
    "# filter data to just columns of interest and MN based\n",
    "traffic_station_df = traffic_station_df[station_vars][traffic_station_df.fips_state_code==27]\n",
    "traffic_df = traffic_df[traffic_vars][traffic_df.fips_state_code==27]\n",
    "\n",
    "# I don't want to carry that super long column name through the project, shorten it\n",
    "traffic_df.rename(columns = {\"traffic_volume_counted_after_0800_to_0900\": \"traffic_volume\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf5ff019769fd1d92f5f3b198ffc69427b138146"
   },
   "source": [
    "The easiest way I can think to identify the traffic stations we want is to plot them on a map. Alternatively, I could employ some regex to search the station_location name... but there is no gaurentee that the station name includes the roadway numbers I'm searching by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c48be8cba01d819287675fd1589bb22064c6516"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import HTML\n",
    "\n",
    "map_osm = folium.Map(location=[44.9778, -93.2650], zoom_start=11)\n",
    "# logitude points need to be negative to map\n",
    "traffic_station_df[\"longitude_2\"] = traffic_station_df[\"longitude\"] * -1\n",
    "\n",
    "traffic_station_df.apply(lambda row:folium.CircleMarker(location=[row[\"latitude\"], row[\"longitude_2\"]], \n",
    "                                              radius=10, popup=row['station_location'])\n",
    "                                             .add_to(map_osm), axis=1)\n",
    "\n",
    "# click on ring of cirlce to see station location name\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33065d7eadfd8b3309cadca019594a2db9aadaee"
   },
   "source": [
    "First major problem: in this dataset there are only traffic stations on one of the 4 roadways we want to predict traffic flow for. For the purporses of this notebook, I'll abbreviate the project to predicting traffic flow for only I-394 just to the west of Minneapolis. Two possible solutions to the problem could be A. find another source of data to supplement this one (maybe Waze provides an open-source dataset?) and/or B. we could build additional models to predict traffic flow X miles away from the traffic station data we do have. I like option A because t[here are certainly more traffic cameras out there](https://hb.511mn.org/#camerasHome?layers=cameras&timeFrame=TODAY)... but probably not pulling flow data. Onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d04366e18dfb44f1182b28a762b75871f880941d"
   },
   "outputs": [],
   "source": [
    "# check out datatypes for traffic station on I-394\n",
    "traffic_df[traffic_df.station_id=='000326'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83db90819ba8bd74fff63badaa3e9f1c2b95def4"
   },
   "source": [
    "For station_id=='000326' we have 730 entries which means we probably have ~1 years worth of data (daily observations for both directions). At this point, it was impossible for me not to try and obtain volume data for this one station from other time periods.. and with a little work [I found 2014 and 2016 on the Minnesotta DOT sight](http://www.dot.state.mn.us/traffic/data/reports-hrvol-atr.html). However, the output is from SAS which means it requires a considerable amount of manual clean up. Plus, I'd have to run my weather API scraper for these years (not a problem, just time consuming since using free sevice). The bank holiday data already includes 2014 and 2016 so that would be no additional work. Since the objective of this notebook is to solidify the foundational skills of a machine learning project, I'm sticking with the smaller dataset (#scopecreep).\n",
    "\n",
    "Since I have narrowed the scope of the project, we don't need to merge the datasets since the traffic_station data will be constant through the entire resulting data. Hence, we'll move on with only a filtered down traffic_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "557714a4161d7b0da003d2bce3783e30b9b64113",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter down dataset to only traffic station on I-394\n",
    "traffic_df = traffic_df[traffic_df.station_id=='000326']\n",
    "# examine numerical features in slimmed dataset\n",
    "traffic_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c00d0e3a2151927edcea69b641e466451ef697bf"
   },
   "source": [
    "Immediately, I realize there are features here that are not needed: fips_state_code is all MN, 'lane_of_travel' is all 0, record type is all 3, and 'restrictions' is empty so I'm dropping those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d46efcf6eaaea3cd1a8fa43090ab637a714bf13"
   },
   "outputs": [],
   "source": [
    "# drop columns we don't want - drop 'em\n",
    "traffic_df.drop(['station_id', 'fips_state_code', 'lane_of_travel',\n",
    "                 'restrictions', 'record_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7badea59797b6424563bc68bf80aac2144658f09"
   },
   "outputs": [],
   "source": [
    "# make sure we have evenly distributed data across the year\n",
    "traffic_df['month_of_data'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13d9464668eb75768ea32bca288f4ad5549aed72"
   },
   "source": [
    "In all months except February, we have 60 (30 days in the month) or 62 (31 days in the month) observations. Next, I contemplated leaving weather data for a future improvement to the model, but weather is so integral to traffic patterns (I think) that I couldn't resist taking the time to write a quick script to pull daily weather data for 2015 in the Minneapolis area from The Weather Underground ([here's the link to that script](https://gist.github.com/FrankRuns/2086a4a5bc9b8c9787d4ddd0453ae971))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52692551d82da65d390fa0ea0977521babaf5d37"
   },
   "outputs": [],
   "source": [
    "# pulled weather data and saved locally as csv\n",
    "weather = pd.read_csv('../input/mn-weather-for-2015-traffic/weather_data.csv')\n",
    "\n",
    "# drop columns with no data\n",
    "weather.drop([\"Unnamed: 0\", \"fog\", \"hail\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adc1a2da487e47869094699b798f773b1ab00c7e"
   },
   "source": [
    "After taking the weather data and dropping 2 columns with no data, I'll join it with our traffic dataset and then create several histograms to explore the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65959012223ed7036aeeb7dda5071941b2f6c480"
   },
   "outputs": [],
   "source": [
    "# join weather data to the traffic volume dataset\n",
    "traffic_df = pd.merge(traffic_df, weather, how='left', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b98964dff76b7ab2d5e16f2bda5474c2385d6c0"
   },
   "outputs": [],
   "source": [
    "# create histograms of each variable\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "traffic_df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "101e0191af308fd85ba1b4148dc804fcd91b95e3"
   },
   "source": [
    "Histograms are not entirely useful for this dataset*. I do see that there are many more 'not-snow' days than 'snow' days. These are also useful for the continuous variables which are temperature (tempi) and the dependent variable 'traffic_volume'. We see a bimodal distribution for traffic_volume with some suspiciously low observations.. which may be holidays. How can we bring in a holiday identifier? [A quick Google search yields this list](https://gist.github.com/shivaas/4758439). Despite the possible inaccuracies for impact on commute volume based on user comments, let's use it and join to our dataset. TODO: come back to me and validate!\n",
    "\n",
    "`* Reminder to self that I'm attempting to replicate Hands-On chapter. Try to stay on track :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f24113dc4d270a30867f46739de9fbb2f351c051"
   },
   "outputs": [],
   "source": [
    "# read in US bank holiday data. Source: https://gist.github.com/shivaas/4758439\n",
    "holidays = pd.read_csv('../input/holidays-for-2015-traffic/holidays.csv', header=0, sep=',', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5ff742133e96f0ba225c02242993a5aa2c3a9b0"
   },
   "outputs": [],
   "source": [
    "holidays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "03b30c76e59e5670d78347ff07dbfe469883fe09"
   },
   "source": [
    "Our holiday dataset is simply a list of US Bank holidays and the date on which they occur which makes it easy for us to create a boolean flag with the traffic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4457ccb9566a6d35df1fd6f81fdebfd31d5d261"
   },
   "outputs": [],
   "source": [
    "# join traffic data with holiday data on the date\n",
    "traffic_df = pd.merge(traffic_df, holidays, how='left', on='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f320b465e8289c8385761b155e8467657220eac"
   },
   "source": [
    "For all rows that do not represent a US bank holiday, we'll create a new feature called 'holiday_flag' and turn those observations into zeros and all other rows into 1's. Thereafter, we'll dump the 'id' and 'holiday_name' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f3fe487ce5fa0c87ec79b858739b2dd3bc82799"
   },
   "outputs": [],
   "source": [
    "traffic_df[['holiday_name']] = traffic_df[['holiday_name']].fillna(value=0)\n",
    "traffic_df['holiday_flag'] = [0 if x == 0 else 1 for x in traffic_df['holiday_name']]\n",
    "traffic_df.drop(['id', 'holiday_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c466ab785e4913ef38f1400a62a8846c6961ea23"
   },
   "outputs": [],
   "source": [
    "# how many days during this time period are US bank holidays?\n",
    "traffic_df[\"holiday_flag\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1430f65bbd9c9c5e087764580db9c4fdf56cc4d"
   },
   "source": [
    "Only 20 days in the dataset are bank holidays. This 'holiday_flag' feature is similar to the ocean_proximity feature in the Hands-On ML dataset we are attempting to mirror. We'll use it now when we split the data into training, testing, and validation with the Stratified Shuffle Split method in SKLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "321c9ee25cae69d3f0f97ae56d9a4e91d45da63a"
   },
   "source": [
    "<a></a>\n",
    "## Create a Test Set\n",
    "<a></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a323aa2ffe7cef588538ce4d1b702dbc76f35fdc"
   },
   "source": [
    "<a></a>\n",
    "### Ensure Test Set is Balanced\n",
    "<a></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd7f549f5db172113e2d6712a99b53014ea63cae"
   },
   "source": [
    "The idea here is to take a subset of our data and stash it away to serve as a final test of the goodness of fit of our model. Without this reference point, there is a good possibility we'll overfit the data and our model will poorly generalize to future data. I am using sklearn's stratified shuffle split to ensure our training set and test set have the same balance of holiday to non-holiday dates in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fefe36baa3a9cad4954ef0666d7c291366617c61"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(traffic_df, traffic_df[\"holiday_flag\"]):\n",
    "    strat_train_set = traffic_df.loc[train_index]\n",
    "    strat_test_set = traffic_df.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c25456b7bb3593a01233b997698c582fdb14930"
   },
   "source": [
    "Since there are relatively few holday dates in our dataset, we want our training data and our testing data to look similar. Imagine if we split our data with all the holiday data going into the testing dataset and none into the training dataset. The holiday flag wouldn't even be considered by any model we produce as an important feature -- and, intuitively we know that since there is generally less traffic volume on holidays than not-holidays we'd be missing a piece of the puzzle. Let's see how the split worked. First, I'll look at the ratio of holiday_flag in the test set and then the entire dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "657dfe57a4758155a25af9e7a684b6015894d6da"
   },
   "outputs": [],
   "source": [
    "strat_test_set[\"holiday_flag\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5c93d5e468751450300e00c7af6343d4324f56d"
   },
   "outputs": [],
   "source": [
    "traffic_df[\"holiday_flag\"].value_counts() / len(traffic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47d2d18878447e025d6e180261364c39462e8368"
   },
   "source": [
    "Perfect. We've set aside a test set to use after we've selected a model. Onward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81021cfc8618aec3fe8a919d9ecfe3039305f8ba"
   },
   "source": [
    "## Discover and Visualize the Data to Gain Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a4c487778441d01745e5a6a88cace35957b65d5"
   },
   "source": [
    "Now, finally, we begin to look for patterns in our data. In Hands-On they look at some visualizations using seaborn and then correlations between the dependent and independant variables.  Start by limiting your dataset to the training data that was split out above (we don't want to look at the test data any more until modeling is complete). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0d541326925b1f9ca4008b1762ffad93bb26e19"
   },
   "outputs": [],
   "source": [
    "# slimming down traffic_df to only our training data\n",
    "traffic_df = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efa2d3e6044cef5426df0d40b9e4272e0741c326"
   },
   "source": [
    "### Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1869fab25059a5f9f6243e8b8e99125cf486234"
   },
   "outputs": [],
   "source": [
    "# what columns do we have at our disposal?\n",
    "traffic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b1f8bfaaa9800da8cd11f47c0fc9b2f300f244f"
   },
   "outputs": [],
   "source": [
    "# seaborn for vis\n",
    "# after running this a few times, became most interested in the impact of holidays on traffic volume\n",
    "import seaborn as sns\n",
    "\n",
    "# view traffic by DOW with impact of holiday flag\n",
    "sns.pairplot(x_vars=[\"day_of_week\"], y_vars=[\"traffic_volume\"],\n",
    "             data=traffic_df, hue=\"holiday_flag\", size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f5432be90ffd01f6c6d20f596de7aae9fa96251"
   },
   "source": [
    "How can we have observations of 0-ish? This feels like bad data. Let's check it out and remove if we think it's an outlier. In this case, I'd be willing to remove these outliers if it was caused by something like construction or.. the Super Bowl because it's impossible to predict those events (at least via this model) in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e14c5ddd97c419ddf35fa422ca9f70065d1d6678"
   },
   "outputs": [],
   "source": [
    "traffic_df[traffic_df.traffic_volume<100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1d996b1a8a999ae15e3cfc74d4da5091ddb54d0"
   },
   "source": [
    "A qucik Google search yielded no obvious events in Minneapolis that week that would cause traffic volume to drop this low. I'm counting it as faulty sensor data and removing as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb7760688fbbe0279e3077f25093a01f0a2b264f"
   },
   "outputs": [],
   "source": [
    "# remove unpredictable outliers\n",
    "traffic_df = traffic_df[traffic_df.traffic_volume>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35f4784ac09bdfc2810a64996067a37160e7a7ed"
   },
   "outputs": [],
   "source": [
    "# checking to see how much data we have left... training set is getting small :(\n",
    "traffic_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ed2fd6d625d92728f078495bc3376e0a565ccde"
   },
   "source": [
    "FYI for later, I want to have a general sense of median traffic volume on any given day..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "755d38f780ba2fc0c4592d3c22de20d233d879e4"
   },
   "outputs": [],
   "source": [
    "print(\"Median traffic volume is \", traffic_df[\"traffic_volume\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50dd10ebc5a38cc38dd6080d7368e9f33fa84054"
   },
   "source": [
    "### Looking for Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d49384d1f814b74bdbe6823365b7a52ba800d69"
   },
   "outputs": [],
   "source": [
    "# let's look at linear correlations\n",
    "corr_matrix = traffic_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31a7f63c47ea6fe80f3a0bb8d8e0d3c8bbe0527e"
   },
   "outputs": [],
   "source": [
    "corr_matrix[\"traffic_volume\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a3439811cc92c8ff729ea29f318f7cdc8c48be4"
   },
   "source": [
    "Unfortunately, nothing is terribly strong here. However, most of these variables are categorical so I wouldn't expect strong correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a95a9f3d3b42635255c7c56fee521dc1ae35d036"
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"traffic_volume\", \"direction_of_travel\", \"day_of_week\",\n",
    "              \"holiday_flag\", \"month_of_data\", \"tempi\", \"snow\", \"visi\"]\n",
    "\n",
    "_ = scatter_matrix(traffic_df[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72cfcf5002f7061b4d2a45c4d86e7505f4c2587e"
   },
   "source": [
    "Again, nothing shocking in the correlation matrix visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c2e82965c6cc5f7dbdd364c0047dd970489f276"
   },
   "source": [
    "### Experimenting with Attribute Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc489efa962db09846d792c9da29eaac43d1ad11"
   },
   "source": [
    "We can attempt to combine features which may retain the level of predictive power and make our model less complex. For example, we may consider adding the snow and rain variabel into a precipitation variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f8ed9456144d82ca22ac6b074b5603e1681e683"
   },
   "outputs": [],
   "source": [
    "# this is the first example of something we'll make reproducible in the Sklean pipeline later\n",
    "traffic_df[\"precip\"] = traffic_df[\"snow\"] + traffic_df[\"rain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4ec8d167036c4c822d3d1a3e954d074bb7e8158"
   },
   "outputs": [],
   "source": [
    "# look at the correlation matrix again\n",
    "corr_matrix = traffic_df.corr()\n",
    "corr_matrix[\"traffic_volume\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5600ddc9c2d98ac95ba8999b2ce251a3376d3a5"
   },
   "source": [
    "Before I split the features from the labels, I'm saving the full dataset in order to calculate benchmark RMSE later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d839166680a0732e2af82978fe2b68df4877d96"
   },
   "outputs": [],
   "source": [
    "traffic_benchmark_data = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb594ed315a16f8764dec37012759980543908a3"
   },
   "source": [
    "## Prepare Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "035dd15657b41c07f392fb46762ff6e5535bcf93"
   },
   "outputs": [],
   "source": [
    "traffic_df = strat_train_set.drop(\"traffic_volume\", axis=1)\n",
    "traffic_labels = strat_train_set[\"traffic_volume\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a4412beb72e9587cbe60ad9cfdc743a92d87524"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "faf8eca73885b8c40438cb7d9356fd83fbfb7f9d"
   },
   "source": [
    "Quoting straight from Hands-On, \"most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them.\" How many missing values does each column contain? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "586ee59dd2d86ec40064f1db378ffb06a38c6904"
   },
   "outputs": [],
   "source": [
    "traffic_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c05d0d2ffe2187419334ec844e8271cf96b3ad8d"
   },
   "source": [
    "Since it's only 2 rows of our dataframe that contains missing values, I'm tempted to simply drop those rows and move on. However, I want to experiment with Scikit-Learn's imputer functions. Since we are filling in the blanks for weather data, it would be foolish to use a simplistic average across the board method and instead should use the average weather for the month in which the missing data points fall. However, the point here is to learn the methods as opposed to building the best possible model. So I'll take median across the board for the numerical features. Where are missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee9f1b4aedb0f4011fef7ad43b403f2d758109aa"
   },
   "outputs": [],
   "source": [
    "traffic_df[traffic_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1542d4f0aa66a9924f3df097ebceceb5e8eac24c"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# subset numerical columns only\n",
    "traffic_num = traffic_df.drop([\"date\", \"conds\"], axis=1)\n",
    "\n",
    "# fit the imputer to numerical data (aka for our case, find the median values in each column)\n",
    "imputer.fit(traffic_num)\n",
    "\n",
    "print(\"Median values:\")\n",
    "for i in range(len(imputer.statistics_)):\n",
    "    print(traffic_num.columns[i], imputer.statistics_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e45496df8a8c5d38441d28462cb65413d836299d"
   },
   "outputs": [],
   "source": [
    "X = imputer.transform(traffic_num)\n",
    "traffic_tr = pd.DataFrame(X, columns=traffic_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87237b8da1bed213d2861fd87b97dd54f0afc5ea"
   },
   "source": [
    "Above, we should have populated the missing values in those 2 rows that we examined above with the median values for the row. Let's check that it worked on move on to fixing the missing values in any categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86a3ae600f7e51b9ef78502ddec35f64bb1b6865"
   },
   "outputs": [],
   "source": [
    "traffic_tr.loc[(traffic_tr.month_of_data==8) & (traffic_tr.day_of_data==6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63d9288748235da50c0de870a0d225d4aa021b44"
   },
   "source": [
    "Got it. Worked. We've taken care of the numerical features, moving on to the categorical ones. There's only one categorical feature we need to fix missing values in: 'conds'. There are 2 missing values we need to handle before converting categorical features into numerical ones using OntHotEncoder. We'll use the most frequently occuring weather condition in the month of August (the 2 missing values are in the month of August) to fill the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a588c50858f0cfdb4571d6afc35ad0680af56603"
   },
   "outputs": [],
   "source": [
    "# finding the average weather condition in August\n",
    "# TODO: this is NOT repeatable when conds=NA across months\n",
    "aug_cond_mode = traffic_df[traffic_df['month_of_data']==8]['conds'].mode()\n",
    "\n",
    "# subset only the 'conds' column\n",
    "traffic_cat = traffic_df[\"conds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52c9fc4747e603c558253f468f9a7cfae9236667"
   },
   "source": [
    "Even though it's time consuming, I have a slight compulsion to always check that the code is doing what I think it should be doing. So, I first find the rows where we have nulls, fill those null values, and then check that the resulting Series is populated with data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c56d86f27fe8d7898bb5d9245bb7c6db75c125cf"
   },
   "outputs": [],
   "source": [
    "traffic_cat[traffic_cat.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d1e3de91883cb4165a7f75ef87d2f07e9c016a2"
   },
   "outputs": [],
   "source": [
    "# fill the missing values\n",
    "traffic_cat.fillna(value=aug_cond_mode[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "358109e3fc20b09ee0c63f540a113f81694141cd"
   },
   "outputs": [],
   "source": [
    "# did we fill in the missing values? \n",
    "traffic_cat[441]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "371b747dec9a615faeade05af447c01f8e1827bd"
   },
   "source": [
    "Perfect. Everything works as expected. That's the pipeline we'll use in the broader preparation pipeline below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c85d1bded3fdb366153c09de8c9e2c5457557b1f"
   },
   "source": [
    "### Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a4ffcc59481690f52d8480c42caab15a0db538a"
   },
   "source": [
    "Below, we transform categorical variables into numeric ones. At first, I used Pandas built in functionality to encode cateogries as numbers and then SciKit Learn's OntHotEncoder to transform those into new binary features. To make the process shorter I used the up-and-coming CategoricalEncoder class from Sklearn to do the 2 aforementioned steps in one fell swoop. Only the later remains below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afa234166719ba451d0428081aadc6e37d1962d9"
   },
   "outputs": [],
   "source": [
    "# Definition of the CategoricalEncoder class, copied from PR #9151.\n",
    "# Source: http://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.CategoricalEncoder.html\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0ff403638560d34493023b05307e5679d949d6c"
   },
   "outputs": [],
   "source": [
    "# Quote from Hands-On:\n",
    "# \"Note that fit_transform() expects a 2D array, but housing_cat\n",
    "# is a 1D array, so we need to reshape it\" traffic_cat is my equivalent to housing_cat\n",
    "traffic_cat_reshaped = traffic_cat.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "350a427a2f90376e0e01c64cc51f74df96169ad2"
   },
   "outputs": [],
   "source": [
    "cat_encoder = CategoricalEncoder(encoding=\"onehot-dense\")\n",
    "traffic_cat_1hot = cat_encoder.fit_transform(traffic_cat_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66d38dea0e6ae71515341e62f483486720d8cb49"
   },
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a3b76823281f4bafb1ff1aac2e16f6e7600cefd"
   },
   "source": [
    "Tip from HandsOn: \"If a categorical attribute has a large number of possible categories (e.g., country code, profession, species, etc.), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. If this happens, you will want to produce denser representations called embeddings, but this requires a good understanding of neural networks (see Chapter 14 for more details).\"\n",
    "\n",
    "Checking in. At this point we have a few dataframes. traffic_df is our original training dataset. traffic_num is a subset of traffic_df of only the numerical features. traffic_tr is a copy of traffic_num with missing weather values imputed using the median strategy and the more manual mode strategy for the 'conds' feature. On the cateogrical side, we start with traffic_cat which is a subset of traffic_df of only the categorical features. Finally, there is traffic_cat_1hot which expands traffic_cat into a wider, sparser dataframe of numerical features. There's a lot happening here and in the next section we wrap all those steps up into Sklearn pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cea6a6e3c7f60e26bd7123798d997b259cf1e418"
   },
   "source": [
    "### Writing Custom Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb6f6aa84454f96bd3f48ef0fc8b236a4a472b19"
   },
   "source": [
    "WIthout a background in software engineering, this was one of the more challenging concepts for me. However, it was well-worth the time figuring this out because it allows me to have a very repeatable pipeline to train and test many models quickly. Below, I build the new class called CombinedWeather and then use it on the traffic_num dataframe to create the new precip variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f889429f2d3683f17037c1db7ad62f1f551c568b"
   },
   "outputs": [],
   "source": [
    "# Good idea to create custom transformer to create 'precip' variable done manually above\n",
    "# Also creating the option to make new_weather_var by multiplying tempi by visi\n",
    "# The output of this class' transform function drops the rain and snow features\n",
    "# since those will get captured in the categorical features one-hot encoding pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# these are the columns of these variables in traffic_num\n",
    "rain_ix, snow_ix, tempi_ix, visi_ix = 4, 5, 6, 7\n",
    "\n",
    "class CombinedWeather(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, combine_weather_vars = True):\n",
    "        self.combine_weather_vars = combine_weather_vars\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        precip = X[:, rain_ix] + X[:, snow_ix]\n",
    "        if self.combine_weather_vars:\n",
    "            new_weather_var = X[:, tempi_ix] * X[:, visi_ix]\n",
    "            X = np.delete(X, np.s_[rain_ix:snow_ix], axis=1)\n",
    "            return np.c_[X , precip, new_weather_var]\n",
    "        else:\n",
    "            X = np.delete(X, np.s_[rain_ix:snow_ix], axis=1)\n",
    "            return np.c_[X, precip]\n",
    "            \n",
    "attr_adder = CombinedWeather(combine_weather_vars = True)\n",
    "traffic_extra_attribs = attr_adder.transform(traffic_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17fa905e2d843d44ed426062fa878b42222f2dcf"
   },
   "outputs": [],
   "source": [
    "# double check column assumption\n",
    "for i in range(len(traffic_num.columns)):\n",
    "    print(traffic_num.columns[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb5c2cbe4fbe5de8f4b9fcf6ded71b99fdd0c81c"
   },
   "outputs": [],
   "source": [
    "# the result is sort of a copy of traffic_num (except in matrix form\n",
    "# instead of a pandas df) with the newly created features\n",
    "# in traffic_num we have 8 columns, in traffic_extra_attribs we should have 10 (new precip and new_weather_var)\n",
    "len(traffic_extra_attribs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "60e4aa9df03225af930f8c516b7c0d3e4b2b26a0"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efde22b5cd5a30bee7f7e70f658afac7b75af7d7"
   },
   "source": [
    "[Some machine learning models will work poorly if the variables are on different scales](https://en.wikipedia.org/wiki/Feature_scaling). As standard practice, we put all of the data on the same scale before training models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d166a60c09bcbe48b8192f0d813528220d1b9d15"
   },
   "outputs": [],
   "source": [
    "# here is traffic_num as it stands currently\n",
    "# notice our newly created features aren't in the dataset - we'll get this in the Sklearn pipelines below\n",
    "traffic_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ede281f7da061c96991ee8189ce5472fcde16257"
   },
   "source": [
    "### Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f63f8abd728ca774700e65352045db9de2a9e733"
   },
   "source": [
    "Here, we'll leverage sklearn's Pipeline to build a pipeline that, first, fills in missing values, then creates new features, and finally puts our data on the same scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc5c1ae56e7e39c9a117591eec407ab8ad15c1bb"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedWeather(combine_weather_vars=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "traffic_num_tr = num_pipeline.fit_transform(traffic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2fa45c4b5ea27a52980484fea4ac60f4f645ef7"
   },
   "outputs": [],
   "source": [
    "# we converted any missing values with the median for that column\n",
    "# then created additional weather related variables\n",
    "# then scaled all features using the standard scaler method (see note below on what this does)\n",
    "# the pipeline takes a pandas dataframe and outputs a numpy array\n",
    "print(len(traffic_num_tr))\n",
    "traffic_num_tr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5622f1e9a15c35355d38100bf97371cd84325142"
   },
   "source": [
    "In the above output, I've printed out the first row of the transformed data. Remember, it should have 10 columns... which it does. The numbers themselves certainly don't look like familiary values... and that's because we've scaled them. What does scaling do?\n",
    "\n",
    "Quotes from HandsOn: \"Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. Standardization is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2823a9ab5264d626254e4e8c0e6e23c4f417e69f"
   },
   "source": [
    "Finally, we want to incorporate all the steps into our Pipeline of starting with a pandas dataframe and going through imputing, feature engineering, and scaling. We create a class below (again, all credit to Hands-On) that helps us feed our original pandas dataframe into our Pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dab68e9cf20e597713057a528dd179f48ef6d1cb"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26e4a831434bb0021a197b88e8e3527eb7e9a555"
   },
   "outputs": [],
   "source": [
    "num_attribs = list(traffic_num)\n",
    "cat_attribs = [\"conds\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedWeather(combine_weather_vars=False)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0731fcc037a02dce0f004aa7795c23ee4212ab60"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a7fc7364be54786df4dab40c48b8dab17a987b8"
   },
   "source": [
    "With everything defined above, we should now be able to take our traffic_df (remember this is the training dataset without the test data or the dependent variable), run it through our Pipeline, and have a dataset that is fully prepared to train machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8834dec7fa1dfd8ff871cd8b66b4e17511cbce2a"
   },
   "outputs": [],
   "source": [
    "traffic_prepared = full_pipeline.fit_transform(traffic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9977a318d03bb72c7642292bff75adc64f9b6dd"
   },
   "source": [
    "Here's the top of our input dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29911dcaa68890ab440bdc2eb3e75b411b8fff9d"
   },
   "outputs": [],
   "source": [
    "traffic_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ababbf5c932d68dc2e02a3c4ca88052e06a62095"
   },
   "source": [
    "And here's the top of our output dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "463d5b8502d4190931b3d03862f8c0acb0102c50"
   },
   "outputs": [],
   "source": [
    "traffic_prepared[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51c4a19d8523d0e36968d2b088afdd399325e55"
   },
   "source": [
    "They look different. The input has 11 columns, but the output has 21. That's because we used One-Hot encoding to transform the categorical 'conds' feature into 12 new independent features. This will allow us to use the categorical information in our machine learning models. Let's do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d789c6170c261da4d4bafbceb4d08e10d5934cd3"
   },
   "source": [
    "## Select and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67f1238fd616d79d48920ec7b2e783f7abf35eab"
   },
   "source": [
    "### Finding a Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "579038d7103ef8dca7c8e19453d643aa26a5af9f"
   },
   "source": [
    "First thing I want to do is create a benchmark RMSE. A prediction model simply using the median traffic volume per weekday and month can help here. Below, we aggregate the data as such and create traffic_volume_y (predictions) and traffic_volume_x (actuals), then use sklearn metrics to calculate the RMSE of those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c69161dada7fcf36a86a158e6a7cf52b48cb6113"
   },
   "outputs": [],
   "source": [
    "dow_median = traffic_benchmark_data.groupby(['day_of_week', 'month_of_data'])['traffic_volume'].median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fe34a1c68e489d4b56d9bed4ca42b1a52a616bb"
   },
   "outputs": [],
   "source": [
    "# join on median flows values for each row based on day of week and month\n",
    "traffic_bench = pd.merge(traffic_benchmark_data, dow_median, how='left', on=['day_of_week','month_of_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6dcad8ef122e58b4a0d3b2e7961153934498aee"
   },
   "outputs": [],
   "source": [
    "# subset the predictions (which is the median traffic volume by dow and month) and actual observed values\n",
    "traffic_bench_preds = traffic_bench['traffic_volume_y']\n",
    "traffic_bench_labels = traffic_bench['traffic_volume_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c673d5cc2f951e8bd8cd75646ad9d8b0bc06163"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "benchmark_mse = mean_squared_error(traffic_bench_labels, traffic_bench_preds)\n",
    "benchmark_rmse = np.sqrt(benchmark_mse)\n",
    "benchmark_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "209f06182d220ccbf766388e66fb744eed06748a"
   },
   "source": [
    "### Training and Evaluating on the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bcee05ef78e0b6432f1ce5340ef76c0a07ae9c9"
   },
   "source": [
    "With a benchmark RMSE = 979 in hand, let's attempt to build some more robust regression models starting with the classic linear regression. The goal now is to build a cross validated model that can give us a signficantly better RMSE than 979... otherwise we'd just use the simple median value method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "240d762b8a9ab32eb7b903671c10fe8e23f93c6e"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(traffic_prepared, traffic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b711f07b1ae64f6ee96c47d8eefd822869697f18"
   },
   "outputs": [],
   "source": [
    "# FYI: when we are ready to predict on the test set (or on new data) we'll need to run\n",
    "# that data through our preparation pipeline. Below is an example of how to do that.\n",
    "some_data = traffic_df.iloc[:5]\n",
    "some_labels = traffic_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25bf911df3dc42e4ff3d5cbedd9663fc47be2217"
   },
   "source": [
    "So, above, we fitted a linear regression model using our prepared data and looked at some sample predictions. Remember from before that the median traffic volume on any given day is about 4,626. The sample predictions seem reasonable. Let's see what the actual values for these observations actually are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9ee8557307a51cd50bda06f43113e2626e9d242"
   },
   "outputs": [],
   "source": [
    "# for the first 5 values of our traffic_df, predictions above and actual values below\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e98c5f73b59d4accdc69504e762ee15d46be5a8"
   },
   "source": [
    "Now they seem less reasonable :) What kind of RMSE does this model give us? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c5ade17e0f73856c2170bfcc4174825c2598cbe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "traffic_predictions = lin_reg.predict(traffic_prepared)\n",
    "lin_mse = mean_squared_error(traffic_labels, traffic_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3914099cccdfc4926a220c4f94dc1eac88702a54"
   },
   "source": [
    "Our linear regression model results with an un-cross validated RMSE of 1,606. This is absolutely not better than using historical averages. Let's try some different types of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15a3ffb5651898e4ef593b440deab2fa4942a3b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(traffic_prepared, traffic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1201f51f24732e706cf037b3b5ded87b1d633e2"
   },
   "outputs": [],
   "source": [
    "traffic_predictions = tree_reg.predict(traffic_prepared)\n",
    "tree_mse = mean_squared_error(traffic_labels, traffic_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ded321da5e03952f3984871151100c52db2b5b0"
   },
   "source": [
    "Hell yeah. RMSE of zero. The model predicts perfectly! If it's too good to be true, you're probably overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "840a791acd1544739cedc2536d8483bc83a6ad70"
   },
   "source": [
    "### Better Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a4841d5524bcd147a77056969616f7de03e97a3"
   },
   "source": [
    "The first decision tree model entirely overfits the data. We should cross validate to understand how this model will generalize to future, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2acf07820e34bd9bea818f40fb4fef56789df282"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, traffic_prepared, traffic_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ad6f0b758085b0f4e3fcbf7cb5c097cb19e4b3b"
   },
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59db5b6bf67d79cc35685d65fbb94782e83fd83f"
   },
   "outputs": [],
   "source": [
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d8018eb177d564d475e58a559e2e80395b7704f"
   },
   "source": [
    "Ok, now we are getting somewhere. Average RMSE of 10 mini-data split and model attempt is 648, significantly better than the benchmark of 979 using the historical average method. Let's go a step further and use many decision trees with the RandomForestRegressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee3e9e46379242418665b11e0a71e6b6c0b70ecf"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(traffic_prepared, traffic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33b30b7d89cff674a948c2850ca468d1dacaead4"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(forest_reg, traffic_prepared, traffic_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec53cfb0ec1092ee87c40ec0a246dbdb092c3b80"
   },
   "outputs": [],
   "source": [
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8b354b807e0d2fe2743c636cf60f556d56d5802"
   },
   "source": [
    "Great. Even better than the single decision tree model with an average RMSE of ~534. Imagine I like that model and want to save it, I can use the following snippit of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acd394970f7fa1c328222f32dd61112f4b53b343"
   },
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(forest_reg, \"forest_reg.pkl\")\n",
    "# and later...\n",
    "# forest_reg = joblib.load(\"forest_reg.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5805fd61fd5f5eb74d01ff146f02b4482c4ad84"
   },
   "source": [
    "## Fine-Tune Hyperparameters with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1e3741ae4d7d4e96760acce2df5bd5bacf18241"
   },
   "source": [
    "There is room to make these models work even better. Different students learn better in different ways, same is true for machine learning models. We can modify the way in which our models learn using hyperparameters. Grid Search in sklearn allows us to try out many different hyperparameter combinations to find the best ones. We'll do that with the Random Forest Decision Tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9837dc50a2a60a73c38152057fae4a9485eb5ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30, 60], 'max_features': [2, 4, 6, 8, 10]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(traffic_prepared, traffic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a8ba27cc239593e5addc0121bd9d47c7d0957cf"
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1e3b6f473fa9187c0302340bb9660dc9e79ffdb"
   },
   "source": [
    "The best model uses the hyperparamters of max_features=10 and n_estimators=60. What is our RMSE for that model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a371c2e2fcf84e835fbbb10f02cdac57b10d5fd3"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(grid_search.best_estimator_, traffic_prepared, traffic_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "gr_forest_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac0f6b54dcf068d6a9e3e666d2925e49303965f2"
   },
   "outputs": [],
   "source": [
    "display_scores(gr_forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14a895fb52763ca5447d7ed5a44e77fcc9bb018b"
   },
   "source": [
    "This turns out to be no better or no worse than using the default parameters for RandomForestRegressor(). For no particular reason, I'll use it going forward to explore the importance of the feautures. \n",
    "\n",
    "When digging into the relative importance of the variables that help us predict traffic volume, here's what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2af8bb091ac52ae29005af0ebb0fea3eaa12eb2"
   },
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "extra_attribs = [\"precip\"]\n",
    "cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "num_attribs = [x for x in traffic_num if x not in [\"rain\"]] # bc removed in CombinedWeather\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a432f3ac09188d60937287295a800cedc601dc6"
   },
   "source": [
    "Ahh. Day of the week and month of the year are the two strongest drivers of traffic volume. That's what we used for the benchmark model that uses historical averages to predict volume. But this model has a better RMSE and that's because it also incorporates direction of travel, temperature, day of the month, and whether the date is a holiday. The remainder of the features don't appear to add much predictive power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da0f2b96457643e1bb1ef90b794fc1e12d17d564"
   },
   "source": [
    "# Evaluate Your System on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "486ac363bca0c4fe286a0c744e9ccfc9ad8a2c32"
   },
   "source": [
    "At this point, I'm OK with where the model is. Time for the final test set. This tells me how well the model will generalize to new data in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3853de68aafdd091f05d3a265be223fa115e8904"
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"traffic_volume\", axis=1)\n",
    "y_test = strat_test_set[\"traffic_volume\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da98c48148f0bfa0101b501103ecf9e4de06373d"
   },
   "outputs": [],
   "source": [
    "# this is a faux pas, but for sake of brevity I'm going with it\n",
    "X_test['conds'].fillna(value=aug_cond_mode[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f08380b0081dbea4841f9ed7fb5025c845dd8067"
   },
   "outputs": [],
   "source": [
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b502e5ddc60ec6919d1c495d0879b03d6ddefc6"
   },
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2fbb1c16c26a79a6d9efe7f8a0e58669780a225"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8752ed2ee955ea416a0414b4ad65103b787c6d45"
   },
   "source": [
    "The final model gave us an RMSE of 479 on our test set. This is significantly lower than our benchmark of 979 (which used historical medians of weekday and month). The final model added information about direction of travel (intiutively super important), temperature (I'm surprised this has more impact than snow), day of month (perhaps like shipping, more people commute at the end of the month), and holiday status (intuitively super important). Looking at average volume by holiday flag indicates to me that people are less likely to commute on holidays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db0b241a38dc4b0e576014ca4926161402fda789"
   },
   "outputs": [],
   "source": [
    "traffic_benchmark_data.groupby(['holiday_flag'])['traffic_volume'].median().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da1ae605252d6b3724095a6d4cd194fb919d000e"
   },
   "source": [
    "There are an abundance of next steps. For starters, the next chapter of Hands-On goes through a similar process but for classiciaftion models (categorical dependent variable). However, as an add-on to the above there are a few immediate possibilities. Find more historical data and try to improve the results. Find historical data for the other roadways in Minneapolis that we wanted to predict traffic volume on. Another idea is to try the same thing for roadways in other parts of the country. Finally, HERE API provides transit time data that we could use to model the relationship between traffic volume and what we all refer to as 'traffic.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2becb28f536db672c10c5ce8e6046af0e439703c"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9647832d6c9f30d1f733b36abd4edfb4919d4f0"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
